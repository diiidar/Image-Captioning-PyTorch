{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e605b969-e460-4a03-adae-6245d8412b2d",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1113610-572c-4c06-a41f-35ea059362bc",
   "metadata": {},
   "source": [
    "<h3>Table of Contents:</h3></br>\n",
    "    &emsp;* Introduction</br>\n",
    "    &emsp;* Dataset</br>\n",
    "    &emsp;* Model Architecture</br>\n",
    "    &emsp;* Training</br>\n",
    "    &emsp;* Evaluation</br>\n",
    "    &emsp;* Encountered Challenges</br>\n",
    "    &emsp;* How To Run the Code</br>\n",
    "    &emsp;* Future Improvements</br>\n",
    "    &emsp;* References</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60dc7c-b22b-42f5-899d-366e1107f4dc",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996e614-da9d-4a7b-ab57-ee223a7e0289",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "<h4>How does the model work?</h4>\n",
    "&emsp;This image captioning model follows an encoder-decoder architecture, where a CNN (Convolutional Neural Network) extracts visual</br>\n",
    "&emsp;features from an image, and an RNN(Recurrent Neural Network) generates a caption based on those features.\n",
    "\n",
    "1. <h5>Feature Extraction (Encoder - CNN):</h5>\n",
    "&emsp;A ResNet model processes the image and extracts visual features.</br>\n",
    "&emsp;The output is a feature vector representing the image content.\n",
    "</br>\n",
    "2. <h5>Caption Generation (Decoder - RNN):</h5>\n",
    "&emsp;The extracted image features are passed to an RNN (LSTM) to generate a sequence of words.\n",
    "&emsp;The decoder predicts the next word in the caption based on the previous words and the image features.\n",
    "&emsp;The process continues until an end token is generated or a maximum length is reached.\n",
    "</br>\n",
    "3. <h5>Training Process:</h5>\n",
    "&emsp;The model is trained using Cross-Entropy Loss, comparing the predicted words with actual captions.\n",
    "&emsp;The Adam optimizer updates the weights to minimize the loss.\n",
    "&emsp;The model learns to associate visual features with meaningful captions through multiple training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3a75f-4614-4080-bfaf-9346a8fe120f",
   "metadata": {},
   "source": [
    "<h4>Key Features of the Approach</h4>\n",
    "&emsp;* CNN-RNN Pipeline: </br>&emsp;&emsp;Uses a ResNet encoder for feature extraction and an LSTM/GRU decoder for caption generation.</br></br>\n",
    "&emsp;* Cross-Entropy Loss: </br>&emsp;&emsp;The loss function ensures that the predicted captions closely match ground truth captions.</br></br>\n",
    "&emsp;* Training with Teacher Forcing: </br>&emsp;&emsp;During training, the model learns from actual captions instead of its own predictions to improve learning speed.</br></br>\n",
    "&emsp;* GPU Acceleration: </br>&emsp;&emsp;The model runs efficiently on GPUs from Kaggle for faster training and inference.</br></br>\n",
    "&emsp;* Checkpoint Saving: </br>&emsp;&emsp;The model periodically saves checkpoints, allowing training to resume if interrupted.</br></br>\n",
    "&emsp;* Data Augmentation (Optional): </br>&emsp;&emsp;Techniques like image flipping or color adjustments can be used to improve generalization.</br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75f3f0-8893-4609-8a9a-4e4971d2c29c",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57523be0-5ad8-4700-b60b-93ba28070f88",
   "metadata": {},
   "source": [
    "<h2>Dataset</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c4b24-1896-4d88-98d7-77096b266d89",
   "metadata": {},
   "source": [
    "<h4>Used dataset for image captioning - Flickr8k</h4>\n",
    "&emsp;* Link to Download dataset : <a href=\"https://www.kaggle.com/datasets/e1cd22253a9b23b073794872bf565648ddbe4f17e7fa9e74766ad3707141adeb\">To Dataset</a>\n",
    "<h4>How the Flickr8k is Structured...</h4>\n",
    "&emsp;* The dataset consists of images paired with corresponding text captions that describe the image content.</br>&emsp;* Each image has 5 captions to provide diverse descriptions.\n",
    "\n",
    "1. Images:\n",
    "</br>Stored in a directory, typically in JPG format.\n",
    "</br>Each image is associated with 5 textual descriptions.</br></br>\n",
    "2. Captions:\n",
    "</br>Stored in a separate annotation file (TXT).\n",
    "</br>Each entry contains the image filename and a corresponding caption.\n",
    "</br></br>Example from Captions file:\n",
    "```python\n",
    "image,caption\n",
    "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
    "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
    "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
    "1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\n",
    "1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d88978-e83f-4fe6-86d6-43bf194728a8",
   "metadata": {},
   "source": [
    "<h4><strong>Preprocessing Steps</strong></h4>\n",
    "&emsp;*Before training, the dataset undergoes several preprocessing steps to ensure consistency and improve model performance.\n",
    "</br>\n",
    "</br>1. Image Preprocessing:\n",
    "</br>* Resizing – Images are resized to a fixed dimension ( 300 x 300 for ResNet ).\n",
    "</br>* Normalization – Pixel values are scaled to [0,1] to match the CNN’s input format.\n",
    "</br>\n",
    "</br>2. Text Preprocessing:\n",
    "</br>* Token Conversion – The caption tokens are converted into a PyTorch tensor.\n",
    "</br>* Vocabulary Building – A word-to-index mapping is created for encoding text.\n",
    "</br>* Padding & Truncation – Captions are padded to a fixed length for batch processing.\n",
    "</br>* Start & End Tokens – Special tokens (#START, #END) are added to mark caption boundaries..\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642e517-288c-43c6-8fba-50d1a55416a7",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8df29-fc21-4dda-85e4-3935bc8a5d51",
   "metadata": {},
   "source": [
    "<h2>Model Architecture</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c80ba-ed51-45f3-accb-b89f1376aa42",
   "metadata": {},
   "source": [
    "*Encoder: CNN (ResNet) extracts image features.\n",
    "</br>*Decoder: RNN (LSTM) generates captions from extracted features.\n",
    "</br>*Loss Function: Cross-Entropy Loss.\n",
    "</br>*Optimizer: Adam.\n",
    "\n",
    "<h4>Why I chose ResNet over simple CNN?</h4>\n",
    "&emsp;1. It solves the Vanishing Gradient Problem, skipping conections preventing gradients from vanishing.</br>\n",
    "&emsp;2. Enables Training of Very Deep Networks\n",
    "\n",
    "<h4>Why I chose LSTM over simple RNN?</h4>\n",
    "&emsp;1. Also solves Vanishing Gradient Problem.</br>\n",
    "&emsp;2. Handles long sequences more efficiently.</br>\n",
    "&emsp;3. Reduces Exploding Gradients.\n",
    "\n",
    "<h4>Why I chose Cross-Entropy as Loss Function?</h4>\n",
    "&emsp;1. It's designed for classification problems. In our case multi-class classification.</br>\n",
    "&emsp;2. Works well with SoftMax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311fccd-207b-4b9d-8524-c48e7d9098fc",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a69ee6-b8ca-49b3-aaee-504427d39863",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c05210-e609-4777-9aa3-d24a8e417b36",
   "metadata": {},
   "source": [
    "<h4>Model Training Loop</h4>\n",
    "* Epoch-based training: Looping over multiple epochs to refine the model.\n",
    "<h4>Forward Pass:</h4>\n",
    "* Image is passed through ResNet to extract features.\n",
    "</br>* Captions are passed through LSTM to generate the next word in the sequence.\n",
    "<h4>Loss Calculation:</h4>\n",
    "* Cross Entropy Loss is used to measure how well predicted captions match the ground truth.\n",
    "<h4>Backpropagation:</h4>\n",
    "* Using Adam Optimizer to adjust weights for both the encoder and decoder.\n",
    "<h4>Checkpointing:</h4>\n",
    "* Model weights are saved every few iterations to prevent losing progress.\n",
    "<h4>Monitoring & Debugging</h4>\n",
    "* Loss Values: Tracking loss over epochs to ensure convergence.\n",
    "<h4>Prediction Checks:</h4>\n",
    "* Generating captions for images every 100 iterations to evaluate performance.\n",
    "<h4>Potential Issues Identified:</h4>\n",
    "* Model predicting the same captions repeatedly (indicating lack of diversity).\n",
    "</br>* Need for better sampling techniques or temperature scaling in the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05955c-0474-4181-a44a-fbb81a9f1ff7",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f8a87-cb37-48e4-bac7-d67be4501da5",
   "metadata": {},
   "source": [
    "<h2>Evaluation</h2>\n",
    "We evaluate the quality of generated image captions using the BLEU (Bilingual Evaluation Understudy) score.\n",
    "</br>BLEU measures how closely the generated caption matches human-written captions by comparing n-grams (sequences of words).\n",
    "</br>In our evaluation, we calculate BLEU scores at different levels to assess the accuracy of generated captions.\n",
    "\n",
    "<h4>Evaluation Process</h4>\n",
    "1. Load the trained model and generate captions for a sample of test images.\n",
    "</br>2. Compare the generated captions with ground truth captions using the BLEU score.\n",
    "</br>3. Aggregate BLEU scores across multiple samples to measure overall performance.\n",
    "\n",
    "<h4>Observations and Challenges</h4>\n",
    "1. BLEU does not account for synonyms or sentence structure, so a meaningful but differently worded caption might receive a low score.\n",
    "</br>2. Shorter captions may score lower, even if they are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045353d-5e8f-45f3-97c7-e89959c8e122",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597b61e-45e7-4da3-b77c-7b4d31e76832",
   "metadata": {},
   "source": [
    "<h2>Encountered Challenges</h2>\n",
    "1. Kaggle Notebook interruption due to inactivity during model training.\n",
    "</br>Solution: Simulate user appearance. Run code in PyCharm that will type something into cell, in background.\n",
    "</br></br>2. Browser Shutting Down while Training, because of Lack of Memory in Browser.\n",
    "</br>Solution: I had to cut half of the dataset, so that training model won't take a lot of time.\n",
    "</br></br>3. Choosing right amount of layers in ResNet, to reduce time.\n",
    "</br>Solution: Tried ResNet50, but GPU couldn't handle it, so I decreased it to ResNet34.\n",
    "</br></br>4. Choosing Batch Size.\n",
    "</br>Solution: Took the maximum the GPU was capable of, which was 21."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c73da5-a1be-4f42-88a2-32ef0ac62292",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00852f2b-0a30-4552-8709-0b95cea21957",
   "metadata": {},
   "source": [
    "<h2>How  to Run the Code</h2>\n",
    "You can run the code row by row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f47f32-a6ab-4012-bde2-d117bdeb5f84",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d8941-7ba4-410c-904c-6dbcf783214b",
   "metadata": {},
   "source": [
    "<h2>Future Improvements</h2>\n",
    "1. Using Transformer-Based models instead of LSTMs;</br>\n",
    "2. There are more advanced CNN models like EfficientNet, ConvNeXt, or Vision Transformer (ViT), which can do feature extraction better.</br>\n",
    "3. Using Cross-Entropy for loss function, instead of losses that correlate better with human evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f796e-4fc3-4777-87b1-c7fd13fe7a68",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad263671-b91b-4cc3-bf91-601633066512",
   "metadata": {},
   "source": [
    "<h2>References</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639b269-dab3-4a93-9f7b-ade81dbc2519",
   "metadata": {},
   "source": [
    "1. https://arxiv.org/pdf/1502.03044v2\n",
    "2. https://www.digitalocean.com/community/tutorials/writing-resnet-from-scratch-in-pytorch\n",
    "3. https://medium.com/@wangdk93/lstm-from-scratch-c8b4baf06a8b\n",
    "4. https://youtu.be/y2BaTt1fxJU?si=10vHpteH-vsX5bFf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
